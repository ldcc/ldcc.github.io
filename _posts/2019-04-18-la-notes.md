---
layout: post
title: Learning notes of Linear Algebra
category: MATHEMATICS
date: 2019-04-18
---

Linear algebra is a tool of mathematics that is widely used throughout science and engineering.
Yet because linear algebra is a form of continuous rather than discrete mathematics,
many computer scientists have inexperience with it.

{% include brline %}

$$
\DeclareMathOperator{infer}{\Gamma \vdash}
\newcommand{norm}[1]{\|#1\|}
\newcommand{vs}[1]{\boldsymbol{\overrightarrow{#1}}}
\newcommand{v}[1]{\boldsymbol{\vec{#1}}}
\newcommand{u}[1]{\boldsymbol{\hat{#1}}}
\newcommand{t}[1]{\boldsymbol{#1}}
$$ Preliminary
{: .hide}

## Unit Vector

The normalized vector $$\u{u}$$ of a non-zero vector $$\v{u}$$ is the unit vector in the direction of vector $$\v{u}$$,

### Proof: 

$$
\sigma ::= \mathbb{R}^n \mid \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix},
\qquad \u{u} \in \sigma, \qquad \norm{\u{u}} = 1  
$$$$$$

where $$\sigma$$ is the type of vector, $$\u{u}$$ means Unit-Vector, $$\norm{\v{u}}$$ is the [Norm][Norm] that is the magnitude of vector,

$$
\frac{\infer \u{u} : \sigma}
     {\infer k\norm{\u{u}} \equiv k}
$$

$$\Gamma$$ is the context care about on algebra, by the way $$\u{u}$$ is a vector, surely $$k\u{u}$$ also is a vector,

$$
\frac{\infer k\norm{\u{u} : \sigma} \equiv k \quad \infer \v{u} : \sigma}
     {\infer \norm{\v{u}} \equiv k}
\qquad \v{u} := k\u{u}
$$

By two step, $$\infer \left(k\norm{\u{u}} \equiv k\right) \wedge \left(\norm{\v{u}} \equiv k\right)$$.

Due to $$\u{u} = \frac{k\u{u}}{k}$$, and thence as we can attempt to do some substitute for this:

$$
\frac{\infer k\norm{\u{u} : \sigma} \equiv \norm{\v{u} : \sigma}}
     {\infer \u{u} \equiv \frac{\v{u}}{k\norm{\u{u}}} \equiv \frac{\v{u}}{\norm{\v{u}}}}
$$

Normally denote as: $$\u{u} \equiv \frac{\v{u}}{\norm{\v{u}}}$$.

Apparently, $$\norm{\v{u}}$$ is a [Scalar][Scalar], and apparently so, $$\u{u}$$ is the unit vector in the direction of non-zero vector $$\v{u}$$.

{% include brline %}

## Linear Equations

Write down a system of linear equations: 

$$
\t{Ax} = \t{b}
$$

$$\t{A}$$ 代表了方程组的系数，$$\t{x}$$ 代表了方程组中的变数，另一侧的 $$\t{b}$$ 则是各组系数与变数之间的线性组合。

For this linear equations:
 
- It is possible to have one exactly solution for every value of $$\ \t{b}$$.
- It is possible to have none or in infinitely many solutions for some values of $$\ \t{b}$$.
- It is not possible to have more than one but less than infinitely many solutions for a particular $$\ \t{b}$$.

{% include brline %}

## Transpose

One important operation on matrices is the transpose, the transpose of a matrix is the mirror image of the matrix across a diagonal line.

$$
\big( \t{A}^\top \big)_{i,j}= \t{A}_{j,i}
$$

The transpose of a matrix product has a simple form:

$$
\big( \t{AB} \big)^\top = \t{B}^\top \t{A}^\top
$$

{% include brline %}

## Identity Matrices (Unit-Matrices)

An identity matrix that is n-dimensional vectors denote as $$\ \t{I}_n$$. 

Formally, $$\ \t{I}_n \in \mathbb{R}^{n \times n}$$:

$$
\t{I}_1 = \begin{bmatrix} 
            1 
          \end{bmatrix}, 
\t{I}_2 = \begin{bmatrix} 
            1 & 0 \\
            0 & 1 \\
          \end{bmatrix},
\t{I}_3 = \begin{bmatrix}
            1 & 0 & 0 \\
            0 & 1 & 0 \\
            0 & 0 & 1 \\
          \end{bmatrix},
\dots,
\t{I}_n = \begin{bmatrix}
            1      & 0      & 0      & \dots  & 0      \\
            0      & 1      & 0      & \dots  & 0      \\
            0      & 0      & 1      & \dots  & 0      \\
            \vdots & \vdots & \vdots & \ddots & \vdots \\
            0      & 0      & 0      & \dots  & 1      \\
          \end{bmatrix}
$$

An identity matrix does not change any vector when it multiply that vector:

$$
\forall \t{x} \in \mathbb{R}^n ,\ \t{I}_n \t{x} = \t{x}
$$

{% include brline %}

## Inverse Matrices

The matrix inverse of $$\ \t{A}$$ is denoted as $$\ \t{A}^{−1}$$, and it is deﬁned as the matrix such that:

$$
\t{A}^{−1}\t{A} = \t{I}_n
$$

We can now solve the linear equation using the following steps:

$$
\begin{split}
\t{Ax}                 & = \t{b}            \\
\t{A}^{-1} \t{Ax}      & = \t{A}^{-1} \t{b} \\
\t{I}_n \t{x}          & = \t{A}^{-1} \t{b} \\
\t{x}                  & = \t{A}^{-1} \t{b} \\
\end{split}
$$

{% include brline %}

## Linear Combination

Think of the columns of $$\t{A}$$ purely as specifying different directions can travel in from the origin.

列向量的意义并非为了描述出一个解，而是为了描述列中各系数间的比值，即关于某个变数的方向。在此观点下，
$$\t{x}$$ 中的标量（即某个变数）就决定了各自所对应的在 $$\t{A}$$ 中的列向量所表示的方向上能走多远。

当得到了一个具体的 $$\t{x}$$ 时，$$\t{A}$$ 的列向量乘以对应标量变数的和便是该线性方程组一个具体的 $$\t{b}$$：

$$
\t{Ax} = \sum_{i} x_i \t{A}_{:,i}
$$

In general, this kind of operation is called a linear combination:

$$
\sum_{i} c_i \t{v}^{(i)},
\qquad \t{v}^{(i)} \in \left\{ \t{v}^{(1)}, \dots, \t{v}^{(n)} \right\}
$$

{% include brline %}

## Span(Generating Subspace)

- A set of vectors can be obtain a solution of a point or vector by linear combination with a particular $$\t{x}$$.

- Similarly, a set of directions can be obtain a solution of space by linear combination with an arbitrary $$\t{x}$$.

And general, this kind of space is called a span.

The span of a set of vectors is the set of all points obtainable by linear combination of those vectors.

{% include brline %}

## Column Space (Range)

In linear algebra, the column space of a matrix $$\t{A}$$ exactly is the span of the column vectors of $$\t{A}$$.

确定 $$\t{Ax} = \t{b} $$ 是否有解相当于确定向量 $$\t{b}$$ 是否在 $$\t{A}$$ 的列向量的生成子空间中。 

{% include brline %}

## Linear Dependence

在讨论线性组合时，矩阵 $$\t{A}$$ 的列向量通常被描述成一个方向，那么当 $$\t{A}$$ 中出现两个**方向**完全相同的列向量时，它们的线性组合又如何呢？
在一个方向上走了一段路后再往同一个方向上继续走，所走过的空间毫无疑问是一条直线。

如果一组向量中某些向量的线性组合可以表示成另一个向量，那么这个向量对这组向量而言便是冗余的，因为这个向量不会增加这组向量的生成子空间。

正式地说，这种冗余被称为线性相关；相反的，如果一组向量的任意向量进行线性组合都不能表示成其他向量时，这组向量称为线性无关。

{% include brline %}

## Norm

Norms is the functions that uses to measure the magnitude of how large a vector is.
Formally, the $$L^p$$ norms given by:

$$
\norm{\t{x}}_p = \left( \sum_i |x_i|^p \right)^{\frac{1}{p}},
\qquad \ p \in \mathbb{R} ,\ p \ge 1
$$

The $$L^1$$ norm, with $$p=1$$, as known as __Manhattan norm__, then $$L^p$$ may simplified to:

$$
\norm{\t{x}}_1 = \sum_i |x_i|
$$

The $$L^2$$ norm, most popular as known as the __Euclidean norm__.
Which is simply the Euclidean distance from the origin to the point identified by x.

If you wish, $$L^2$$ norm could be more intuitive to understanding by [Pythagorean Theorem][Pythagorean]:

![lan][lan]
{:.center}

Think about the vector $$\vs{AC}$$ is generated by the $$\vs{AB}$$ and $$\vs{BC}$$, and so on:

$$
\vs{AC} = \vs{AB} + \vs{BC} = \begin{bmatrix}
  \norm{\vs{AB}} \\
  \norm{\vs{BC}}
\end{bmatrix}
$$

By Pythagorean:

$$
\norm{\vs{AC}}_2
= \sqrt{\norm{\vs{AB}}^2 + \norm{\vs{BC}}^2}
= \left( \sum_i |AC_i|^2 \right)^{\frac{1}{2}}
$$

The $$L^\infty$$ norm, with $$p \rightarrow \infty$$, as known as __Maximum Norm__:

$$
\norm{\t{x}}_\infty = \max_i\left( |x_1| \right)
$$

下面是被不同范数所描述的单位圆，单位圆上的每一点到原点的距离均相等：

![unic][unic]
{:.center}


(?<=[a-zA-Z])\} ?+\\t\{
{:.hide}

{% assign res = page.path | slice: 18,20 | remove: ".md" | prepend: site.res %}

[lan]: {{res}}/lan.svg
[unic]: {{res}}/unic.svg

[Norm]: https://en.wikipedia.org/wiki/Norm_(mathematics)
[Scalar]: https://en.wikipedia.org/wiki/Scalar_(mathematics)
[Pythagorean]: https://en.wikipedia.org/wiki/Pythagorean_theorem